
# 18/8/2025

Total probability
![[Pasted image 20250818193106.png]]

Probability after sence
![[Pasted image 20250818200615.png]]

Normalized Distribution
![[Pasted image 20250818201332.png]]

![[Pasted image 20250818202623.png]]

Visualizing our code(for code see localization.py in notes)
![[Pasted image 20250818205733.png]]

What we have learnt till now:

### ğŸš© What your `sense` function is doing

- You start with a **prior distribution** `p` (your belief about where you might be before sensing).
- Then you take a **measurement** `Z` (like â€œredâ€ or â€œgreenâ€).
- The function updates the prior using **Bayesâ€™ rule**:

p(Xiâˆ£Z)â€…â€Š=â€…â€Šp(Zâˆ£Xi)â€‰p(Xi)âˆ‘jp(Zâˆ£Xj)â€‰p(Xj)p(X_i \mid Z) \;=\; \frac{p(Z \mid X_i)\,p(X_i)}{\sum_j p(Z \mid X_j)\,p(X_j)}p(Xiâ€‹âˆ£Z)=âˆ‘jâ€‹p(Zâˆ£Xjâ€‹)p(Xjâ€‹)p(Zâˆ£Xiâ€‹)p(Xiâ€‹)â€‹

This new distribution is called the **posterior distribution**.

---
### ğŸŒ Intuition in plain words
You are calculating:
> â€œGiven my prior belief about where I am, and given the new sensor reading Z, what is my updated belief about where I am?â€

- **Prior** = â€œBefore seeing the sensor, I thought all positions were equally likely.â€
- **Likelihood** = â€œHow likely is it to see `Z` (red/green) if I were at each position?â€ (this is what `pHit` and `pMiss` encode).
- **Posterior** = â€œAfter considering the evidence, hereâ€™s my new belief distribution over positions.â€

**Bayesian Sense Function (Robot Localization)**
- **p** â†’ Prior distribution: belief about position before sensing (e.g., `[0.2,0.2,0.2,0.2,0.2]`).
- **world** â†’ Map of the environment (e.g., `['green','red','red','green','green']`).
- **Z** â†’ Sensor measurement/observation (e.g., `'red'`).
- **pHit** â†’ Weight applied if measurement matches the world (sensor correct).
- **pMiss** â†’ Weight applied if measurement does not match the world (sensor incorrect).

**Process:**
1. Multiply each prior probability by `pHit` if `world[i] == Z`, else by `pMiss`.
2. Normalize so the new probabilities sum to 1.
3. The result is the **posterior distribution**: updated belief after sensing.

**Formula:**

![[Pasted image 20250818210637.png]]
ğŸ‘‰ In short: _Update your belief about position using the sensor reading._


Nice â€” youâ€™ve just extended your code to handle **multiple sensor measurements** in sequence ğŸ‰

Letâ€™s break down exactly whatâ€™s happening:

---



### ğŸ” What are we trying to do?

Weâ€™re trying to figure out **where the robot is**, given a noisy sensor. The robot sees â€œredâ€ or â€œgreen,â€ but the sensor isnâ€™t perfect:
- If it sees **red**, thereâ€™s a **60% chance itâ€™s actually red**, but also a **40% chance itâ€™s wrong**.
- If it sees **green**, thereâ€™s a **20% chance itâ€™s actually green**, and an **80% chance itâ€™s wrong**.

So every time the robot makes a measurement, we should trust it **a little**, but not completely.

---
### âš™ï¸ What happens step by step?
1. **Start with ignorance (prior):**  
    Initially, the robot has no idea where it is â†’ probability is spread **uniformly** across all cells (each cell = 20%).
    
2. **Take a measurement (say â€œredâ€):**
    - If the robot sees â€œred,â€ we should lean towards cells that are red.
    - But we donâ€™t fully trust it â†’ so we only increase probability for red cells by 0.6 and for green cells by 0.2.
    
3. **Normalize (renormalize total = 1):**  
    After this weighting, the total probability may not equal 1. Normalizing just rescales things so probabilities sum to 100%.
    - Now, red cells have **higher probability** (because the sensor said â€œredâ€).
    - Green cells still have **some probability** (because the sensor might be wrong).
    
4. **Repeat with more measurements:**  
    Each new measurement keeps shifting probabilities.
    - If the sensor keeps saying â€œred,â€ confidence in red cells grows stronger.
    - If it flips to â€œgreen,â€ probabilities shift towards green cells.

---
### ğŸ¤¯ The intuition
The robot is **updating its belief about the world**.
- **Before measurement:** â€œI could be anywhere equally.â€
- **After measurement:** â€œSince my sensor said â€˜red,â€™ Iâ€™m more likely in a red spot. But Iâ€™ll still keep some chance for green, because my sensor might be lying.â€

Itâ€™s like a detective who gets **clues**: each clue doesnâ€™t prove the answer, but together they narrow down the possibilities.

---
ğŸ‘‰ So, when we normalize probabilities, weâ€™re not saying the robot is definitely in green or red.  
Weâ€™re saying:
- â€œGiven the evidence so far, how should I **weigh** the chances of being in each location?â€

# 19/8/2025

Move function (Exact motion, in a cyclic world)
![[Pasted image 20250819195321.png]]

Inaccurate Robot motion
![[Pasted image 20250819195551.png]]


Here what we are trying to say is a robot wants to move U steps, the probability that it moves U steps is 0.8, the probability it overshoots by 1 step is 0.1 and the probability it undershoots by 1 is also 0.1
![[Pasted image 20250819200257.png]]
If the first cells diagram is the initial diagram, after the robots motion, the second diagram is the updated motion.

example 2
![[Pasted image 20250819201247.png]]

Example 3
![[Pasted image 20250819201643.png]]
In this case it is a special case where, it will still give the same 0.2 for each cell. If we consider movement U=2 for every cells' value, we will get (0.02 + 0.16 + 0.02 = 0.2) in every cell.

Intution
Letâ€™s think of this as a story of a little robot moving around a circular world ğŸŒ:

---
### ğŸ”¹ Deterministic Move (the simple case)

If the robot starts at position 1 with 100% certainty (`[0,1,0,0,0]`) and you tell it _â€œmove 2 steps forwardâ€_, then it **always ends up at exactly position 3**.  
So after the move, the probability distribution is:

`[0,0,0,1,0]`

Super neat, super clean. But real robots arenâ€™t that perfect.
---
### ğŸ”¹ Why Inexact Moves?

Imagine your robot has:
- **Slippery wheels**
- **Uneven floor**
- **Sensors with delay**

So when you say _â€œmove 2 steps forwardâ€_:
- **Most of the time (0.8)** â†’ it lands where you expect (exact move).
- **Sometimes (0.1)** â†’ it overshoots, rolling one extra step.
- **Sometimes (0.1)** â†’ it undershoots, stopping short by one step.

The robot is **unreliable**, so you hedge your bets with probabilities.

---
### ğŸ”¹ Intuition Behind the Distribution
Think of the distribution as **our belief about where the robot might be**.
- When we start: `[0,1,0,0,0]` â†’ _â€œIâ€™m 100% sure the robot is at position 1.â€_
- After the move: `[0,0,0.1,0.8,0.1]` â†’  
    _â€œI strongly believe the robot is at position 3, but thereâ€™s a small chance it slipped and is at 2 or 4.â€_
    
This mirrors how **humans think about uncertainty**: we donâ€™t say â€œitâ€™s exactly here,â€ we say â€œitâ€™s most likely here, but maybe there too.â€

---
### ğŸ”¹ Uniform Distribution Case (p3)
If the robot starts with a **completely uncertain position**: `[0.2,0.2,0.2,0.2,0.2]`,  
then moving doesnâ€™t really help â€” because every position spreads its uncertainty equally.  
So the distribution stays uniform.
This matches intuition:
- If you had _no clue_ where the robot was, moving it doesnâ€™t magically give you more information.
---
### ğŸ”¹ Big Picture Intuition
- **Move function** spreads the probability cloud, because motion is unreliable.
- **Sense function** squeezes the cloud, because sensing gives us sharper clues.
- Alternating move + sense = the robot gradually **homes in on where it must be**, despite noise.
---
ğŸ‘‰ So the intuition:
- **Move = uncertainty grows** (the cloud gets blurrier).
- **Sense = uncertainty shrinks** (the cloud gets sharper).
- Together, they balance into **robust localization**.


## Limit Distribution
![[Pasted image 20250819204740.png]]

**Limit Distribution in a Cyclic World**  
When a robot moves in a cyclic world with inexact motion (overshoot/undershoot probabilities), each movement redistributes probability across neighboring positions. If the robot keeps moving indefinitely, the uncertainty spreads out until every position is equally likely. At this point, the distribution reaches **balance**, where no position is favored over another. This balanced state is called the **limit distribution**, and it is always **uniform** (e.g., `[0.2, 0.2, 0.2, 0.2, 0.2]` for 5 cells).

Mathematical intution

**Limit Distribution in a Cyclic World**  
When a robot moves in a cyclic world with inexact motion (with probabilities of exact, overshoot, and undershoot), its belief distribution spreads out over time. After many moves, the distribution reaches a **balanced state** where each cell receives the same probability mass from its neighbors.

Mathematically, for any cell XiX_iXiâ€‹:

p(Xi)=0.8â‹…p(Xiâˆ’1)+0.1â‹…p(Xiâˆ’2)+0.1â‹…p(Xi)p(X_i) = 0.8 \cdot p(X_{i-1}) + 0.1 \cdot p(X_{i-2}) + 0.1 \cdot p(X_{i})p(Xiâ€‹)=0.8â‹…p(Xiâˆ’1â€‹)+0.1â‹…p(Xiâˆ’2â€‹)+0.1â‹…p(Xiâ€‹)

In this balanced state, all p(Xi)p(X_i)p(Xiâ€‹) are equal, so the only possible solution is a **uniform distribution** (e.g., `[0.2, 0.2, 0.2, 0.2, 0.2]` for 5 cells).

Thus, if the robot keeps moving forever in a cyclic world, the distribution always converges to **uniform**.

## IMP: Move- Sense Cycle
![[Pasted image 20250819212936.png]]

### **1. Move (Prediction step)**

- When the robot **moves**, it becomes _less certain_ about its exact position.
- Example: If the robot thinks itâ€™s at cell 3 and tries to move 1 step, because of noise (overshoot/undershoot), it might actually end up in cell 2, 3, or 4.
- This spreads out the probability distribution â†’ the belief becomes _blurred_.
- **Effect:** The robot **loses information** (entropy increases).
---
### **2. Sense (Update step)**
- When the robot **senses**, it compares the sensor measurement with the actual world.
- If the sensor says â€œred,â€ then all red cells get higher probability, and all non-red cells get lower probability.
- This sharpens the distribution â†’ the belief becomes more concentrated.
- **Effect:** The robot **gains information** (entropy decreases).
---
### **3. The Cycle**
- The robot alternates between **move (loses certainty)** and **sense (gains certainty)**.
- Over time, this balance lets the robot track its position, even in noisy worlds.
---
ğŸ‘‰ Intuition:
- **Move = spreading out belief (uncertainty grows).**
- **Sense = correcting belief (uncertainty shrinks).**

In **probabilistic robotics** (or more generally, Bayesian filtering):

- **Motion step (prediction/update using control):**
    - When the robot moves, we add _uncertainty_ because motion is never perfect.
    - The probability distribution â€œspreads out.â€
    - That means the **entropy (uncertainty)** of the belief increases.

- **Measurement step (correction/update using sensors):**
    - When the robot senses the environment, the measurement gives information that â€œsharpensâ€ the distribution.
    - The belief becomes more _peaked_ (less spread out).
    - That means the **entropy decreases** â€” uncertainty goes down.
![[Pasted image 20250819220324.png]]

So, the process is like:

ğŸ”„ **Motion â†’ entropy â†‘ (less certain)**  
ğŸ” **Measurement â†’ entropy â†“ (more certain)**

Example 1 (Refer coding example 11)
![[Pasted image 20250819214202.png]]
Looking at this example, the robot sensed red, moved right, sensed green and moved right, by looking at the world and comparing, the robot must most likely be in cell number 5 (last cell, green). And by seeing the results, we see probability of the last cell green in 0.38 which is the highest, so robot is most likely there!. 

Example 2 (refer coding example 2 in localization.py)

Localization summary
![[Pasted image 20250819220016.png]]



# Formal Definition of Probability 1

Baye's rule 
p (Xi/Z) is posterior probability distribution
p(Xi) - prior belief
p(Z/Xi) - measurement probability (the weight attached - pHit/pMiss)
p(Z) - normalized sum

![[Pasted image 20250819222025.png]]

To make it more formalized
![[Pasted image 20250819222406.png]]

p bar is non normalized probability, alpha is sum of all non normalized probability vector.