


Total probability
![[Pasted image 20250818193106.png]]

Probability after sence
![[Pasted image 20250818200615.png]]

Normalized Distribution
![[Pasted image 20250818201332.png]]

![[Pasted image 20250818202623.png]]

Visualizing our code(for code see localization.py in notes)
![[Pasted image 20250818205733.png]]

What we have learnt till now:

### ğŸš© What your `sense` function is doing

- You start with a **prior distribution** `p` (your belief about where you might be before sensing).
- Then you take a **measurement** `Z` (like â€œredâ€ or â€œgreenâ€).
- The function updates the prior using **Bayesâ€™ rule**:

p(Xiâˆ£Z)â€…â€Š=â€…â€Šp(Zâˆ£Xi)â€‰p(Xi)âˆ‘jp(Zâˆ£Xj)â€‰p(Xj)p(X_i \mid Z) \;=\; \frac{p(Z \mid X_i)\,p(X_i)}{\sum_j p(Z \mid X_j)\,p(X_j)}p(Xiâ€‹âˆ£Z)=âˆ‘jâ€‹p(Zâˆ£Xjâ€‹)p(Xjâ€‹)p(Zâˆ£Xiâ€‹)p(Xiâ€‹)â€‹

This new distribution is called the **posterior distribution**.

---
### ğŸŒ Intuition in plain words
You are calculating:
> â€œGiven my prior belief about where I am, and given the new sensor reading Z, what is my updated belief about where I am?â€

- **Prior** = â€œBefore seeing the sensor, I thought all positions were equally likely.â€
- **Likelihood** = â€œHow likely is it to see `Z` (red/green) if I were at each position?â€ (this is what `pHit` and `pMiss` encode).
- **Posterior** = â€œAfter considering the evidence, hereâ€™s my new belief distribution over positions.â€

**Bayesian Sense Function (Robot Localization)**
- **p** â†’ Prior distribution: belief about position before sensing (e.g., `[0.2,0.2,0.2,0.2,0.2]`).
- **world** â†’ Map of the environment (e.g., `['green','red','red','green','green']`).
- **Z** â†’ Sensor measurement/observation (e.g., `'red'`).
- **pHit** â†’ Weight applied if measurement matches the world (sensor correct).
- **pMiss** â†’ Weight applied if measurement does not match the world (sensor incorrect).

**Process:**
1. Multiply each prior probability by `pHit` if `world[i] == Z`, else by `pMiss`.
2. Normalize so the new probabilities sum to 1.
3. The result is the **posterior distribution**: updated belief after sensing.

**Formula:**

![[Pasted image 20250818210637.png]]
ğŸ‘‰ In short: _Update your belief about position using the sensor reading._


Nice â€” youâ€™ve just extended your code to handle **multiple sensor measurements** in sequence ğŸ‰

Letâ€™s break down exactly whatâ€™s happening:

---



### ğŸ” What are we trying to do?

Weâ€™re trying to figure out **where the robot is**, given a noisy sensor. The robot sees â€œredâ€ or â€œgreen,â€ but the sensor isnâ€™t perfect:
- If it sees **red**, thereâ€™s a **60% chance itâ€™s actually red**, but also a **40% chance itâ€™s wrong**.
- If it sees **green**, thereâ€™s a **20% chance itâ€™s actually green**, and an **80% chance itâ€™s wrong**.

So every time the robot makes a measurement, we should trust it **a little**, but not completely.

---
### âš™ï¸ What happens step by step?
1. **Start with ignorance (prior):**  
    Initially, the robot has no idea where it is â†’ probability is spread **uniformly** across all cells (each cell = 20%).
    
2. **Take a measurement (say â€œredâ€):**
    - If the robot sees â€œred,â€ we should lean towards cells that are red.
    - But we donâ€™t fully trust it â†’ so we only increase probability for red cells by 0.6 and for green cells by 0.2.
    
3. **Normalize (renormalize total = 1):**  
    After this weighting, the total probability may not equal 1. Normalizing just rescales things so probabilities sum to 100%.
    - Now, red cells have **higher probability** (because the sensor said â€œredâ€).
    - Green cells still have **some probability** (because the sensor might be wrong).
    
4. **Repeat with more measurements:**  
    Each new measurement keeps shifting probabilities.
    - If the sensor keeps saying â€œred,â€ confidence in red cells grows stronger.
    - If it flips to â€œgreen,â€ probabilities shift towards green cells.

---
### ğŸ¤¯ The intuition
The robot is **updating its belief about the world**.
- **Before measurement:** â€œI could be anywhere equally.â€
- **After measurement:** â€œSince my sensor said â€˜red,â€™ Iâ€™m more likely in a red spot. But Iâ€™ll still keep some chance for green, because my sensor might be lying.â€

Itâ€™s like a detective who gets **clues**: each clue doesnâ€™t prove the answer, but together they narrow down the possibilities.

---
ğŸ‘‰ So, when we normalize probabilities, weâ€™re not saying the robot is definitely in green or red.  
Weâ€™re saying:
- â€œGiven the evidence so far, how should I **weigh** the chances of being in each location?â€


